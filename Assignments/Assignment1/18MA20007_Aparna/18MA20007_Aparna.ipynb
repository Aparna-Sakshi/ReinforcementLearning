{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKQquN4dlFu8"
      },
      "source": [
        "# **CS60077: Reinforcement Learning**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Homework 01**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Y3nwwyleVv"
      },
      "source": [
        "---\n",
        "**Student Details:** (Make sure to fill your details before submitting, \n",
        "\n",
        "---\n",
        "**Name:** \n",
        "\n",
        "```\n",
        "Aparna Sakshi\n",
        "```\n",
        "\n",
        "**Roll Number:** \n",
        "\n",
        "```\n",
        "18MA20007\n",
        "```\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4L26uv8Qs2b"
      },
      "source": [
        "The Assignment Consists of Two parts. The first part consists of a couple of descriptive questions. These questions are written in the notebook itself with space provided to add your answer. It is encouraged to type your answers in the notebook itself utilising the inbuilt $\\LaTeX$ commands. But you may write these answers on a sheet of paper, scan it and upload it with this assignment. The second part consists of a coding task where you need to implement the Dynamic Programming algorithms you learnt in class (Policy Evaluation, Policy Iteration and Value Iteration) on a simple RL environment called ***Frozen Lake***. We have written some portions of the code and you only need to fill in your implementation of the algorithms in the space provided. \n",
        "\n",
        "**Submission Instruction:** Submit the completed `.ipynb` notebook and a pdf containing the solutions to the descriptive questions (only if you haven't written it in the notebook) to moodle. Naming convention: `<RollNumber>_<FirstName>.ipynb`. \n",
        "\n",
        "*We encourage students NOT to upload any other file. Uploading unnecessary files may lead to penalties.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqgh7eJORzRe"
      },
      "source": [
        "---\n",
        "---\n",
        "`**DESCRIPTIVE TASK**` Answer the following questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpcvNCUMTmPO"
      },
      "source": [
        "[**Question 1**]: Let $v(s)$ and $q(s,a)$ denote the state and action value functions respectively for an MDP where $s\\in\\mathcal{S}$ denotes a state and $a\\in\\mathcal{A}$ denotes an action. $\\mathcal{S}$ and $\\mathcal{A}$ are the state space and action space respectively. Assume $\\mathbf{v}$ and $\\mathbf{q}$ are vectors of lengths $|\\mathcal{S}|$ and $|\\mathcal{S}|\\cdot|\\mathcal{A}|$ respectively [Note that you can think of $\\mathbf{q}$ as a vector where the number of elements in such a vector is $|\\mathcal{S}|\\cdot|\\mathcal{A}|$]. Let $\\tilde{\\mathbf{q}}$ is any vector in $\\mathbb{R}^{|\\mathcal{S}|\\cdot|\\mathcal{A}|}$ denoting a legitimate action value function and $\\mathbf{q}^*$ is the optimal value function for the MDP. Let $\\pi$ be the greedy policy w.r.t.  $\\tilde{\\mathbf{q}}$. Then prove the following.\n",
        "\\begin{equation}\n",
        "\tv^*(s) - v_\\pi(s) \\leq 2||\\mathbf{q}^* - \\tilde{\\mathbf{q}}||_\\infty + \\gamma ||\\mathbf{v}^* - \\mathbf{v}_\\pi||_\\infty\n",
        "\\end{equation}\n",
        "where, $\\gamma$ is the discount factor, $v_\\pi(s)$ is the state-value function corresponding to the greedy policy $\\pi$ and $v^*(s)$ is the optimal value function. Similarly, $\\mathbf{v}_\\pi$ and $\\mathbf{v}^*$ are the vector of all value functions for policy $\\pi$ and the optimal policy respectively.\n",
        "\n",
        "**For your convenience the following starter is provided**:\n",
        "\n",
        "\\begin{align}\n",
        "v^*(s) - v_\\pi(s)  &= v^*(s) - q^*(s,\\pi(s)) + q^*(s,\\pi(s)) - v_\\pi(s) \\\\\n",
        "&= v^*(s) - q^*(s,\\pi(s)) + q^*(s,\\pi(s)) - \\tilde{q}(s,\\pi(s)) \\quad [\\text{ since } v_\\pi(s)=\\tilde{q}(s,\\pi(s))] \\\\\n",
        "&= v^*(s) - q^*(s,\\pi(s)) \\cdots \\text{use Bellman eqn. for the last two terms above and relate the }\\\\\n",
        "& \\text{resulting expression to the infinity norm of the difference between } \\mathbf{v}^* \\text{ and } \\mathbf{v}_\\pi\n",
        "\\end{align}\n",
        "\n",
        "Now try to manipulate the first two terms of the above, as follows.\n",
        "\\begin{align}\n",
        "v^*(s) - q^*(s,\\pi(s))  &= v^*(s) - \\tilde{q}(s,\\pi(s)) + \\tilde{q}(s,\\pi(s)) - q^*(s,\\pi(s)) \\\\\n",
        "&= q^*(s,\\pi^*(s)) - \\tilde{q}(s,\\pi(s)) + \\tilde{q}(s,\\pi(s)) - q^*(s,\\pi(s))\\\\\n",
        "&\\text{For the last two terms use the fact that an element of a vector is less than}\\\\\n",
        "&\\text{the infinity norm of the vector.}\\\\\n",
        "&\\text{For the first two terms also you have to use a similar trick, but for that }\\\\\n",
        "&\\tilde{q}(s,\\pi(s)) \\text{ needs to be replaced with } \\tilde{q}(s,\\pi^*(s))\\\\\n",
        "&\\text{by properly using the inequality between the two}.\n",
        "\t\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oTL2ExRTqKE"
      },
      "source": [
        "*Your Answer for Question 1 Here ...* Double-click to edit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPKH5swTTtgB"
      },
      "source": [
        "[**Question 2**]: You have already proved the policy evaluation for the modifed Bellman Operator for action value function in your Quiz 2. To remind you, the modifed Bellman Operator was defined as:\n",
        "\\begin{equation}\n",
        "T^{\\pi} Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim p}[V(s_{t+1})]\n",
        "\\end{equation}\n",
        "where the modified value function $V(s_t)$ is defined as, \n",
        "\\begin{equation}\n",
        "V(s_t) = \\mathbb{E}_{a_t \\sim \\pi}[Q(s_t, a_t) - \\text{log} \\pi(a_t|s_t)]\n",
        "\\end{equation}\n",
        "\n",
        "Now consider to policy update operation defined as, \n",
        "\\begin{equation}\n",
        "\\pi_{\\text{new}} = \\text{argmin}_{\\pi^{'} \\in \\Pi}D_{KL} \\Big( \\pi^{'}(\\cdot | s_t) \\Big| \\Big| \\frac{\\text{exp}(Q^{\\pi_{\\text{old}}}(s_t, \\cdot))}{Z^{\\pi_{\\text{old}}}(s_t)} \\Big)\n",
        "\\end{equation}\n",
        "where $\\Pi$ is the set of all policies, $Z^{\\pi^{\\text{old}}}(s_t)$ is the partition function used to normalise the distribution and $D_{KL}$ is the Kullback-Leiblar Divergence. The KL-Divergence between two distributions $p$ and $q$ is defined as, \n",
        "\\begin{equation}\n",
        "D_{KL}(p || q) = \\mathbb{E}_{p}\\Big[\\text{log}\\frac{p}{q} \\Big] = \\int p(x) \\text{log}\\frac{p(x)}{q(x)}dx\n",
        "\\end{equation}\n",
        "\n",
        "You need to prove that $Q^{\\pi_{\\text{new}}}(s_t, a_t) \\ge Q^{\\pi_{\\text{old}}}(s_t, a_t)$ for all $(s_t, a_t) \\in S \\times A$. Assume $|A| < \\infty$.\n",
        "\n",
        "***Hint***: Define the function $J$ as, \n",
        "\\begin{equation}\n",
        " J^{\\pi_{\\text{old}}}(\\pi) = D_{KL} \\Big( \\pi(\\cdot | s_t) \\Big| \\Big| \\frac{\\text{exp}(Q^{\\pi_{\\text{old}}}(s_t, \\cdot))}{Z^{\\pi_{\\text{old}}}(s_t)} \\Big)\n",
        " \\end{equation}\n",
        "\n",
        "You should agree with the fact the $J^{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) \\le J^{\\pi_{\\text{old}}}(\\pi_{\\text{old}})$. Now simply expand $J$ using the definition of KL-Divergence and simplify to get an upper bound on $V^{\\pi_{\\text{old}}}(s_t)$. Using this bound and the modified Bellman Operator, repeatedly expand $Q^{\\pi_{\\text{old}}}$ and iteratively apply policy evaluation operation i.e. $Q^{k+1} = T^{\\pi}Q^{k}$ to prove $Q^{\\pi_{\\text{new}}}(s_t, a_t) \\ge Q^{\\pi_{\\text{old}}}(s_t, a_t)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOT6P9Djb0P2"
      },
      "source": [
        "*Your Answer for Question 2 Here ...* Double-click to edit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaWOoclVihnQ"
      },
      "source": [
        "---\n",
        "---\n",
        "`**CODING TASK**` You need to implement value iteration and policy iteration for the Frozen Lake environment from [OpenAI Gym](https://gym.openai.com/envs/FrozenLake-v0). We have provided custom versions of this environment in the starter code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXxadMSJjNCo"
      },
      "source": [
        "**How to Start?**\n",
        "\n",
        "*   Upload the shared zip file (`CS60077_HW1_StarterCodes.zip`) to Colab and you are good to go for executing the cells below!\n",
        "\n",
        "\n",
        "*Please do NOT change the structure of the code. We expect you to write your codes in the desired segment mentioned with comments ONLY.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZunEUyskFAA"
      },
      "source": [
        "Unzip the uploaded file by executing the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7WrlcP-kEpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c51629c-af54-4eb5-912f-c77a694191ad"
      },
      "source": [
        "!unzip CS60077_HW1_StarterCodes.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  CS60077_HW1_StarterCodes.zip\n",
            "  inflating: discrete_env.py         \n",
            "  inflating: frozen_lake.py          \n",
            "  inflating: lake_envs.py            \n",
            " extracting: requirements.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1rGIeakV4r"
      },
      "source": [
        "Install all the necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YGGtbKFhATW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54a4c76-2036-431e-d868-27e9f5c5965f"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.10.9\n",
            "  Downloading gym-0.10.9.tar.gz (1.5 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 235 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 266 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 276 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 286 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 327 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 348 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 368 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 378 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 399 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 409 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 419 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 440 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 450 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 460 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 471 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 491 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 501 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 512 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 522 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 532 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 542 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 552 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 563 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 583 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 593 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 604 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 614 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 634 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 645 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 655 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 665 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 675 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 686 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 696 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 706 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 727 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 737 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 747 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 757 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 768 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 778 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 788 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 798 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 808 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 819 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 829 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 839 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 849 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 860 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 870 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 880 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 890 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 901 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 911 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 921 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 931 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 942 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 952 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 962 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 972 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 983 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 993 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.0 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.9->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.9->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.9->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym==0.10.9->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.9->-r requirements.txt (line 1)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.9->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.9->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.9->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.9-py3-none-any.whl size=1587025 sha256=57e6c0b05773b79763a98df7fd5109dd8a8e076ab40f964b12c758d792aa3060\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/3d/73/929b7d38e163e1ed40ebb4400fc9a10533221fe694935a52ee\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.10.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voNMcbcXkab9"
      },
      "source": [
        "Import the required modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGAQ3iA6fjTK"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "from lake_envs import *\n",
        "\n",
        "np.set_printoptions(precision=3)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-F4FlIHgADl"
      },
      "source": [
        "For the functions (defined below) `policy_evaluation`, `policy_improvement`, `policy_iteration` and `value_iteration`,\n",
        "the parameters `P`, `nS`, `nA`, `gamma` are defined as follows:\n",
        "\n",
        "\tP: nested dictionary\n",
        "\t\tFrom gym.core.Environment\n",
        "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
        "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
        "\t\t\t- probability: float\n",
        "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
        "\t\t\t- nextstate: int\n",
        "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
        "\t\t\t- reward: int\n",
        "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
        "\t\t\t\t\"nextstate\" with \"action\"\n",
        "\t\t\t- terminal: bool\n",
        "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
        "\tnS: int\n",
        "\t\tnumber of states in the environment\n",
        "\tnA: int\n",
        "\t\tnumber of actions in the environment\n",
        "\tgamma: float\n",
        "\t\tDiscount factor. Number in range [0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUaLnxG_lsxh"
      },
      "source": [
        "# action-value function\n",
        "def q(s,a,v,P,gamma):\n",
        "  return sum([p*(r+gamma*v[s_]) for p,s_,r,isgoal in P[s][a]])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXn0qb6_f74r"
      },
      "source": [
        "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
        "\t\"\"\"Evaluate the value function from a given policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\tpolicy: np.array[nS]\n",
        "\t\tThe policy to evaluate. Maps states to actions.\n",
        "\ttol: float\n",
        "\t\tTerminate policy evaluation when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns\n",
        "\t-------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\t\tThe value function of the given policy, where value_function[s] is\n",
        "\t\tthe value of state s\n",
        "\t\"\"\"\n",
        "\n",
        "\tvalue_function = np.zeros(nS)\n",
        "\n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\n",
        "\twhile True:\t\n",
        "\t\te=0\n",
        "\t\tfor s in range(0,nS):\n",
        "\t\t\tv = value_function[s]\n",
        "\t\t\ta = policy[s] #action\n",
        "\t\t\tvalue_function[s] = q(s,a,value_function,P,gamma)\n",
        "\t\t\te = max(e, abs(v-value_function[s]))\n",
        "\t\tif e < tol:\n",
        "\t\t\tbreak\n",
        "\t############################\n",
        "\treturn value_function"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuLPbTpgTNM"
      },
      "source": [
        "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
        "\t\"\"\"Given the value function from policy improve the policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\tvalue_from_policy: np.ndarray\n",
        "\t\tThe value calculated from the policy\n",
        "\tpolicy: np.array\n",
        "\t\tThe previous policy.\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnew_policy: np.ndarray[nS]\n",
        "\t\tAn array of integers. Each integer is the optimal action to take\n",
        "\t\tin that state according to the environment dynamics and the\n",
        "\t\tgiven value function.\n",
        "\t\"\"\"\n",
        "\n",
        "\tnew_policy = np.zeros(nS, dtype='int')\n",
        "\n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\tfor s in range(0,nS):\n",
        "\t\t# action-value pair for each action\n",
        "\t\tq_s = np.array([q(s,a,value_from_policy,P,gamma) for a in range(0,nA)])\t\t\n",
        "\t\tnew_policy[s] = np.argmax(q_s)\n",
        "\t############################\n",
        "\treturn new_policy\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Z8abzugVoZ"
      },
      "source": [
        "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
        "\t\"\"\"Runs policy iteration.\n",
        "\n",
        "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
        "\timplement this method.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\ttol parameter used in policy_evaluation()\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "\t\"\"\"\n",
        "\n",
        "\tvalue_function = np.zeros(nS)\n",
        "\tpolicy = np.zeros(nS, dtype=int)\n",
        "\n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\twhile True:\t\n",
        "\t\tvalue_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
        "\t\tnew_policy = policy_improvement(P, nS, nA, value_function, policy, gamma=0.9)\n",
        "\t\tif (new_policy==policy).all():\n",
        "\t\t\tbreak\n",
        "\t\telse:\n",
        "\t\t\tpolicy=new_policy\n",
        "\n",
        "\t############################\n",
        "\treturn value_function, policy"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaRLOIG2gaNq"
      },
      "source": [
        "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
        "\t\"\"\"\n",
        "\tLearn value function and policy by using value iteration method for a given\n",
        "\tgamma and environment.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\tTerminate value iteration when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "\t\"\"\"\n",
        "\n",
        "\tvalue_function = np.zeros(nS)  \n",
        "\tpolicy = np.zeros(nS, dtype=int)\n",
        "  \n",
        "\t############################\n",
        "\t# YOUR IMPLEMENTATION HERE #\n",
        "\tn=0\n",
        "\twhile True:\t\n",
        "\t\te=0\n",
        "\t\tfor s in range(0,nS):\n",
        "\t\t\tv = value_function[s]\t\t\t\n",
        "\t\t\tq_s = np.array([q(s,a,value_function,P,gamma) for a in range(0,nA)])\t\t\n",
        "\t\t\tvalue_function[s] = np.max(q_s)\n",
        "\t\t\te = max(e, abs(v-value_function[s]))\n",
        "\t\tn+=1\n",
        "\t\tif e < tol:\n",
        "\t\t\tbreak\n",
        "\n",
        "\tfor s in range(0,nS):\n",
        "\t\t# action-value pair for each action\n",
        "\t\tq_s = np.array([q(s,a,value_function,P,gamma) for a in range(0,nA)])\t\t\n",
        "\t\tpolicy[s] = np.argmax(q_s)\t\t\t\t\n",
        "\t############################\n",
        "\treturn value_function, policy"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-aKU3Bga0q"
      },
      "source": [
        "def render_single(env, policy, max_steps=100):\n",
        "  \"\"\"\n",
        "    This function does not need to be modified\n",
        "    Renders policy once on environment. Watch your agent play!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      Environment to play on. Must have nS, nA, and P as\n",
        "      attributes.\n",
        "    Policy: np.array of shape [env.nS]\n",
        "      The action to take at a given state\n",
        "  \"\"\"\n",
        "\n",
        "  episode_reward = 0\n",
        "  ob = env.reset()\n",
        "  for t in range(max_steps):\n",
        "    env.render()\n",
        "    time.sleep(0.25)\n",
        "    a = policy[ob]\n",
        "    ob, rew, done, _ = env.step(a)\n",
        "    episode_reward += rew\n",
        "    if done:\n",
        "      break\n",
        "  env.render();\n",
        "  if not done:\n",
        "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
        "  else:\n",
        "  \tprint(\"Episode reward: %f\" % episode_reward)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sagRbH33k4Su"
      },
      "source": [
        "Let's test now!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7iCcCVKglE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e2487e-36f6-4bce-e0d0-3ab6fbb4c4c0"
      },
      "source": [
        "# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
        "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
        "# env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "render_single(env, p_pi, 100)\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "render_single(env, p_vi, 100)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------\n",
            "Beginning Policy Iteration\n",
            "-------------------------\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode reward: 1.000000\n",
            "\n",
            "-------------------------\n",
            "Beginning Value Iteration\n",
            "-------------------------\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode reward: 1.000000\n"
          ]
        }
      ]
    }
  ]
}